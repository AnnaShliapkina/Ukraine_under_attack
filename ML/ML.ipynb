{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9868217054263566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Extract additional features from the date\n",
    "data['event_date'] = pd.to_datetime(data['event_date'])  # Convert the 'date' column to datetime format\n",
    "data['day'] = data['event_date'].dt.day  # Day\n",
    "data['month'] = data['event_date'].dt.month  # Month\n",
    "data['year'] = data['event_date'].dt.year  # Year\n",
    "\n",
    "# Drop the original 'date' column\n",
    "data.drop('event_date', axis=1, inplace=True)\n",
    "\n",
    "# Convert categorical variables to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "data['region_encoded'] = label_encoder.fit_transform(data['region'])\n",
    "data['actor1_encoded'] = label_encoder.fit_transform(data['actor1'])\n",
    "data['actor2_encoded'] = label_encoder.fit_transform(data['actor2'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = data[['region_encoded', 'day', 'month', 'year', 'actor1_encoded', 'actor2_encoded', 'fatalities']]  # Select the features you want to use\n",
    "y = data['event_type']  # Target variable (event type)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=3000)  # Initialize the model\n",
    "model.fit(X_train, y_train)  # Train the model on the training data\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model investigated the classification of event types based on various parameters available in the dataset. It aimed to predict the type of event (e.g., Shelling/artillery/missile attack, Air/drone strike) using features such as region, date, actors involved, and fatalities. By analyzing these factors, the model aimed to understand the patterns and relationships that influence the occurrence and classification of events.\n",
    "\n",
    "The accuracy of the model is approximately 98.68%, indicating a high level of agreement between the predicted and actual classes in the test dataset. This suggests that the model has effectively learned the relationships between the input features and the output classes. However, it's advisable to consider other performance metrics, such as precision, recall, and F1-score, especially if the classes in the dataset are imbalanced. Overall, the model demonstrates strong predictive capability based on the provided data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess model performance in scenarios with imbalanced data, additional effectiveness metrics such as:\n",
    "\n",
    "1. Precision: It represents the proportion of correctly classified instances of a specific class out of all instances predicted by the model for that class. It's computed as:\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "where TP denotes true positives and FP stands for false positives.\n",
    "\n",
    "2. Recall: This metric measures the ratio of correctly classified instances of a specific class to the total instances of that class in the original dataset. It's calculated as:\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "where FN denotes false negatives.\n",
    "\n",
    "3. F1-score: It's the harmonic mean of precision and recall, offering a balanced evaluation of the model. The formula is:\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Explosions/Remote violence       0.99      1.00      0.99     15275\n",
      "                     Riots       1.00      0.00      0.00         1\n",
      "Violence against civilians       0.56      0.02      0.05       204\n",
      "\n",
      "                  accuracy                           0.99     15480\n",
      "                 macro avg       0.85      0.34      0.35     15480\n",
      "              weighted avg       0.98      0.99      0.98     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Getting model predictions for the test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Printing a summary report with classification metrics\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By the precision metric, the model has high values for the \"Explosions/Remote violence\" and \"Riots\" classes (99% and 100% respectively), but it has significantly lower precision for the \"Violence against civilians\" class (56%). This means that the model correctly identifies events of \"Explosions/Remote violence\" and \"Riots\" types but is less effective in recognizing \"Violence against civilians\".\n",
    "\n",
    "Regarding the recall metric, the model also exhibits high values for the \"Explosions/Remote violence\" and \"Violence against civilians\" classes (100% and 2% respectively), but a low value for the \"Riots\" class (0%). This indicates that the model effectively captures events of \"Explosions/Remote violence\" and \"Violence against civilians\" types but struggles to recognize \"Riots\".\n",
    "\n",
    "The F1-score, being a harmonic mean between precision and recall, also reflects the model's high precision for the \"Explosions/Remote violence\" and \"Violence against civilians\" classes but inefficiency for the \"Riots\" class.\n",
    "\n",
    "Taking all these metrics into account, we can conclude that the model performs well for certain classes but faces challenges with others. It's crucial to understand the reasons behind these issues and possibly refine the model or explore alternative approaches to improve its effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
